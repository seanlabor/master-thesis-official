{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d6aaf0cfee1945348c1f62d411510a91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5a225cab2eb34b0fab423481d729af03",
              "IPY_MODEL_c885ae0813b64e32bc375c877c845819",
              "IPY_MODEL_0ab9b9a2d5df4198a925725482fa7a70"
            ],
            "layout": "IPY_MODEL_0734144f53da4abb9c92300f74d62699"
          }
        },
        "5a225cab2eb34b0fab423481d729af03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8504d52345f9480fa0419321ba7f4d19",
            "placeholder": "​",
            "style": "IPY_MODEL_494f4ce884a14e4d9d55a42fe4ca6093",
            "value": " 10%"
          }
        },
        "c885ae0813b64e32bc375c877c845819": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d716a4032d3422ea085f19cce9fa4e9",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b29be8a50e704c92bd4ba10a2b24de65",
            "value": 5
          }
        },
        "0ab9b9a2d5df4198a925725482fa7a70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ef4c724ca2e47da822a2dd0329ccdda",
            "placeholder": "​",
            "style": "IPY_MODEL_6831ad6e6c824be580fe04f115a46bfd",
            "value": " 5/50 [05:26&lt;43:13, 57.64s/it]"
          }
        },
        "0734144f53da4abb9c92300f74d62699": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8504d52345f9480fa0419321ba7f4d19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "494f4ce884a14e4d9d55a42fe4ca6093": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d716a4032d3422ea085f19cce9fa4e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b29be8a50e704c92bd4ba10a2b24de65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6ef4c724ca2e47da822a2dd0329ccdda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6831ad6e6c824be580fe04f115a46bfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "hEuN9OKrGN0F"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "from jax.scipy.special import logsumexp\n",
        "import jax\n",
        "from numpy.random.mtrand import noncentral_chisquare\n",
        "from torch._C import NoneType\n",
        "from tqdm.notebook import tqdm\n",
        "import _pickle as cPickle\n",
        "import pickle\n",
        "import copy\n",
        "from jax import jit, vmap, pmap, grad, value_and_grad\n",
        "import random\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.datasets import KMNIST\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torchvision.datasets import EMNIST\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from torch.utils.data import DataLoader\n",
        "from jax.example_libraries import stax, optimizers\n",
        "import torchvision\n",
        "import torch\n",
        "from numpy.random import default_rng\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import torch.utils.data as data_utils\n",
        "from jax.flatten_util import ravel_pytree\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import _pickle as cPickle\n",
        "from math import e\n",
        "import time\n",
        "from jax.example_libraries import stax\n",
        "from jax.example_libraries.stax import Dense, Relu, LogSoftmax\n",
        "from sklearn.model_selection import train_test_split\n",
        "from jax import random\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "from logging import Logger\n",
        "import time\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zme13D7hbQxF",
        "outputId": "1285ac4a-ec10-4112-895d-6e25f8e611ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "FP3lGeSxvsO5"
      },
      "outputs": [],
      "source": [
        "known_npy = ['anvil',\n",
        " 'bathtub',\n",
        " 'bicycle',\n",
        " 'baseball bat',\n",
        " 'book',\n",
        " 'arm',\n",
        " 'asparagus',\n",
        " 'airplane',\n",
        " 'blackberry',\n",
        " 'bee',\n",
        " 'blueberry',\n",
        " 'barn',\n",
        " 'apple',\n",
        " 'banana',\n",
        " 'bench',\n",
        " 'bear',\n",
        " 'bandage',\n",
        " 'The Great Wall of China',\n",
        " 'basketball',\n",
        " 'basket']\n",
        "\n",
        "unknown_npy = ['beach',\n",
        " 'aircraft carrier',\n",
        " 'boomerang',\n",
        " 'axe',\n",
        " 'beard',\n",
        " 'bat',\n",
        " 'bird',\n",
        " 'ambulance',\n",
        " 'animal migration',\n",
        " 'The Mona Lisa',\n",
        " 'binoculars',\n",
        " 'bed',\n",
        " 'birthday cake',\n",
        " 'angel',\n",
        " 'alarm clock',\n",
        " 'belt',\n",
        " 'baseball',\n",
        " 'The Eiffel Tower',\n",
        " 'ant',\n",
        " 'backpack']\n",
        "\n",
        "\n",
        "if not any(\".npy\" in item for item in known_npy):\n",
        "  known_npy = [x+\".npy\" for x in known_npy]\n",
        "\n",
        "if not any(\".npy\" in item for item in unknown_npy):\n",
        "  unknown_npy = [x+\".npy\" for x in unknown_npy]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "hjKZxBoYvTQK"
      },
      "outputs": [],
      "source": [
        "'''Load data from npy files and normalize and standardize\n",
        "train_npy and test_npy set manually'''\n",
        "\n",
        "\n",
        "\n",
        "data_image_size = 28\n",
        "\n",
        "def normalize_standardize(arr):\n",
        "  arr = np.array(arr)/255\n",
        "  arr = (arr-arr.mean())/arr.std()\n",
        "  return arr\n",
        "\n",
        "\n",
        "x_train_known_dataset = []\n",
        "x_test_known_dataset = []\n",
        "y_train_known_dataset = []\n",
        "y_test_known_dataset = []\n",
        "\n",
        "x_train_unknown_dataset = []\n",
        "x_test_unknown_dataset = []\n",
        "y_train_unknown_dataset = []\n",
        "y_test_unknown_dataset = []\n",
        "\n",
        "known_list = []\n",
        "unknown_list = []\n",
        "\n",
        "data_image_size = 28\n",
        "data_image_depth = 1\n",
        "number_datapoints_npy = 5000\n",
        "#data_filepath = \"./Quickdraw_data/\"\n",
        "data_filepath = \"/content/drive/MyDrive/Colab Notebooks/Handdraw/data/\"\n",
        "\n",
        "\n",
        "\n",
        "if not any(\".npy\" in item for item in known_npy):\n",
        "  known_npy = [x+\".npy\" for x in known_npy]\n",
        "\n",
        "for idx, files in enumerate(os.listdir(data_filepath)):\n",
        "  if \"npy\" in files:\n",
        "    file_path = data_filepath+files\n",
        "    npy_load = np.load(file_path, encoding = 'latin1', allow_pickle = True)\n",
        "    rng = default_rng(idx)\n",
        "    idx_list = rng.choice(len(npy_load), size = number_datapoints_npy*2, replace = False)\n",
        "    idx_list_train = idx_list[number_datapoints_npy:]\n",
        "    idx_list_test = idx_list[:number_datapoints_npy]\n",
        "\n",
        "\n",
        "    if files in known_npy:\n",
        "\n",
        "      x_train_known_dataset.extend(npy_load[idx_list_train])\n",
        "      x_test_known_dataset.extend(npy_load[idx_list_test])\n",
        "      y_train_known_dataset.extend([idx]*number_datapoints_npy)\n",
        "      y_test_known_dataset.extend([idx]*number_datapoints_npy)\n",
        "      known_list.append(idx)\n",
        "\n",
        "    if files in unknown_npy:\n",
        "      x_train_unknown_dataset.extend(npy_load[idx_list_train])\n",
        "      x_test_unknown_dataset.extend(npy_load[idx_list_test])\n",
        "      y_train_unknown_dataset.extend([idx]*number_datapoints_npy)\n",
        "      y_test_unknown_dataset.extend([idx]*number_datapoints_npy)\n",
        "      unknown_list.append(idx)\n",
        "\n",
        "x_train_known_dataset = normalize_standardize(x_train_known_dataset).reshape(-1,data_image_size,data_image_size)\n",
        "y_train_known_dataset = np.array(y_train_known_dataset)\n",
        "x_test_known_dataset = normalize_standardize(x_test_known_dataset).reshape(-1,data_image_size,data_image_size)\n",
        "y_test_known_dataset = np.array(y_test_known_dataset)\n",
        "\n",
        "\n",
        "y_train_unknown_dataset = np.array(y_train_unknown_dataset)\n",
        "x_train_unknown_dataset = normalize_standardize(x_train_unknown_dataset).reshape(-1,data_image_size,data_image_size)\n",
        "y_test_unknown_dataset = np.array(y_test_unknown_dataset)\n",
        "x_test_unknown_dataset = normalize_standardize(x_test_unknown_dataset).reshape(-1,data_image_size,data_image_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwG6IZfAO9Y6"
      },
      "source": [
        "## **Funktions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXbzoV3qDgWh"
      },
      "source": [
        "Changes in Jax Jit function (sometimes?) need a cache restart for changes to be applied. Happened with def accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "-nXolMEPadfs"
      },
      "outputs": [],
      "source": [
        "def pathandstuff():\n",
        "\n",
        "    global save_txt\n",
        "    global base_path\n",
        "    global save_path\n",
        "\n",
        "    if os.path.exists(googledrive_path):\n",
        "        print(\"on google\")\n",
        "        base_path = googledrive_path\n",
        "    else:\n",
        "        raise ValueError('Please create folder for save path or connect to Google Drive')\n",
        "        \n",
        "    logs_path = base_path\n",
        "    '''Set logging and temp paths'''\n",
        "    timestamp = time.strftime(\"%d.%m.%Y_%H.%M\")\n",
        "    foldername = timestamp\n",
        "    save_path = os.path.join(logs_path,foldername,)\n",
        "    save_path = save_path+\"/\"\n",
        "    save_txt = os.path.join(save_path, 'Log_Jax_MNist_{}.txt'.format(foldername))\n",
        "      \n",
        "    if use_pickle:\n",
        "      save_path = pickle_path.split(\"best_weight_\")[0] \n",
        "      for i in os.listdir(save_path):\n",
        "        if \"Log\" in i:\n",
        "          save_txt = os.path.join(save_path, i)\n",
        "\n",
        "    print(\"Log path:\",save_path)\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "    return save_path\n",
        "\n",
        "'''Save current notebook'''\n",
        "def logg_script(file_name, save_path):\n",
        "  source = f\"/content/drive/MyDrive/Colab Notebooks/{file_name}.ipynb\"\n",
        "  destination = save_path+f\"{file_name}.ipynb\"\n",
        "  shutil.copy2(source, destination)\n",
        "\n",
        "'''logging to txt and print'''\n",
        "def logg(string_, array = None):\n",
        "  if array is None:\n",
        "\n",
        "    file1 = open(save_txt,\"a\")\n",
        "    file1.write(string_)\n",
        "    file1.write(\"\\n\")\n",
        "    file1.close()\n",
        "    print(string_)\n",
        "  if array is not None:\n",
        "\n",
        "    file1 = open(save_txt,\"a\")\n",
        "    file1.write(string_)\n",
        "    file1.write(str(array))\n",
        "    file1.write(\"\\n\")\n",
        "    file1.close()\n",
        "    print(string_, array)\n",
        "\n",
        "def log_variables():\n",
        "    try:\n",
        "    \n",
        "      logg((\"n_training_epochs = {}\".format(n_training_epochs)))\n",
        "      logg((\"n_offsp_epoch = {}\".format(n_offsp_epoch)))\n",
        "      logg((\"n_samples = {}\".format(n_samples)))\n",
        "      logg((\"n_test = {}\".format(n_test)))\n",
        "      logg((\"batch_size = {}\".format(batch_size)))\n",
        "      logg((\"std_modifier = {}\".format(std_modifier)))\n",
        "      logg((\"use_pickle = {}\".format(use_pickle)))\n",
        "      logg((\"pickle_path = {}\".format(pickle_path)))\n",
        "      logg((\"NNin1 = {}\".format(NNin1)))\n",
        "      logg((\"NNout1 = {}\".format(NNout1)))\n",
        "      logg((\"Convu_in1 = {}\".format(Convu1_in)))\n",
        "      logg((\"Convu2_in = {}\".format(Convu2_in)))\n",
        "      logg((\"kernelsize_ = {}\".format(kernelsize_)))\n",
        "      logg((\"n_metaepochs = {}\".format(n_metaepochs)))   \n",
        "      logg((\"n_offsp_epoch = {}\".format(n_offsp_epoch)))\n",
        "      logg((\"n_offsprings = {}\".format(n_offsprings)))\n",
        "    except Exception:\n",
        "      pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "RC9y3pZqqbwg"
      },
      "outputs": [],
      "source": [
        "def logg_GPU():\n",
        "  try:\n",
        "    affe=!nvidia-smi\n",
        "    string=\"\"\n",
        "    for i in affe:\n",
        "      string+=i\n",
        "    logg(\"GPU:\",string.split(\"=========||   0  \")[1].split(\"            Off  | 000\")[0])\n",
        "  except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Np2DOGve_e5E"
      },
      "outputs": [],
      "source": [
        "'''creating offsprings, filling treeleaf of 0 and 1 with gaussian noise, doesnt seem to be a problem, ex in offspring_list[0][5]'''\n",
        "\n",
        "\n",
        "def jax_create_offsprings(key,n_offspr,  fath_weights,std_var):\n",
        "  statedic_list = []\n",
        "  randomizer_list = []\n",
        "  for i in range(0,n_offspr):\n",
        "    std_modifier = std_var\n",
        "    rng = jax.random.PRNGKey(key+i)\n",
        "    random_value_tree = tree_random_normal_like(rng,fath_weights,std_modifier)\n",
        "    son = jax.tree_map(lambda x,y: x+y, fath_weights,random_value_tree)\n",
        "    statedic_list.append(son)\n",
        "    return statedic_list, []\n",
        "\n",
        "def random_split_like_tree(rng_key, target = None, treedef = None):\n",
        "    if treedef is None:\n",
        "        treedef = jax.tree_util.tree_structure(target)\n",
        "    keys = jax.random.split(rng_key, treedef.num_leaves)\n",
        "    return jax.tree_util.tree_unflatten(treedef, keys)\n",
        "\n",
        "\n",
        "def tree_random_normal_like(rng_key, target,std_modifier):\n",
        "    keys_tree = random_split_like_tree(rng_key, target)\n",
        "    return jax.tree_map(\n",
        "        lambda l, k: jax.random.normal(k, l.shape, l.dtype)*std_modifier,\n",
        "        target,\n",
        "        keys_tree,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Le4KpG9snz40"
      },
      "outputs": [],
      "source": [
        "'''softmax for offspring list\n",
        "    checked 11.04 working correctly'''\n",
        "def softmax_offlist(off_list,acc_list,temp):\n",
        "  softmax_list = softmax_result(acc_list,temp)\n",
        "  for i in range(len(off_list)):\n",
        "    if i ==  0:\n",
        "      top_dog = jax.tree_map(lambda x: x*softmax_list[i], off_list[i])\n",
        "    else:\n",
        "      general_dog = jax.tree_map(lambda x: x*softmax_list[i], off_list[i])\n",
        "      top_dog = jax.tree_map(lambda x,y: x+y, top_dog,general_dog)\n",
        "  return top_dog\n",
        "\n",
        "\n",
        "'''Creates softmax/temp list out of accuracy list [0.2,0.3,....,0.8]'''\n",
        "def softmax_result(results,temp: float):\n",
        "    x = [z/temp for z in results]\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "F7PIGc8Y-zsc"
      },
      "outputs": [],
      "source": [
        "def sigma_decay(start, end, n_iter):\n",
        "  return(end/start)**(1/n_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "rgqKtdgZ-GZF"
      },
      "outputs": [],
      "source": [
        "def cat_dataloader(x,y,random_categories, random_state, n_samples, n_test, number_different_catruns):\n",
        "  x_train = np.empty((n_samples*number_different_catruns,data_image_size,data_image_size))\n",
        "  x_test = np.empty((n_test*number_different_catruns,data_image_size,data_image_size))\n",
        "  y_train = np.empty((n_samples*number_different_catruns,))\n",
        "  y_test = np.empty((n_test*number_different_catruns,))\n",
        "\n",
        "  for i in range(number_different_catruns):\n",
        "\n",
        "    '''Choose 5 random categories from data'''\n",
        "    x_data0 = x[np.isin(y, random_categories[i]).flatten()]\n",
        "    y_data0 = y[np.isin(y, random_categories[i]).flatten()]\n",
        "\n",
        "    '''relabel categories to 0-5 to avoid accuracy bug in mlp'''\n",
        "    map_dic = dict(zip(random_categories[i], list(range(number_training_categories))))\n",
        "    y_data0 = np.vectorize(map_dic.get)(y_data0)\n",
        "\n",
        "    x_train_temp, x_test_temp, y_train_temp, y_test_temp = train_test_split(x_data0, y_data0, train_size = n_samples,\n",
        "                                                  test_size = n_test,stratify = y_data0,\n",
        "                                                  random_state = random_state)\n",
        "\n",
        "    x_train[i*n_samples:(i+1)*n_samples] = x_train_temp\n",
        "    x_test[i*n_test:(i+1)*n_test] = x_test_temp\n",
        "    y_train[i*n_samples:(i+1)*n_samples] = y_train_temp\n",
        "    y_test[i*n_test:(i+1)*n_test] = y_test_temp\n",
        "\n",
        "  '''Cast to Jax Array'''\n",
        "  x_train = jnp.array(x_train,dtype = \"float32\").reshape(len(x_train), -1)\n",
        "  x_test = jnp.array(x_test,dtype = \"float32\").reshape(len(x_test), -1)\n",
        "  y_train = jnp.array(y_train)\n",
        "  y_test = jnp.array(y_test)\n",
        "\n",
        "  '''Reshape for Jax Vectorization'''\n",
        "  x_train = x_train.reshape(-1,int((n_samples/batch_size)),batch_size,data_image_size,data_image_size)\n",
        "  y_train = y_train.reshape(-1,int((n_samples/batch_size)),batch_size)\n",
        "  x_test = x_test.reshape(-1,n_test,data_image_size,data_image_size)\n",
        "  y_test = y_test.reshape(-1,n_test)\n",
        "\n",
        "  return x_train, x_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "G4NrxSVjKt8f"
      },
      "outputs": [],
      "source": [
        "def init_MLP(layer_widths, parent_key, scale = 0.01):\n",
        "\n",
        "    params = []\n",
        "    keys = jax.random.split(parent_key, num = len(layer_widths)-1)\n",
        "\n",
        "    for in_width, out_width, key in zip(layer_widths[:-1], layer_widths[1:], keys):\n",
        "        weight_key, bias_key = jax.random.split(key)\n",
        "        params.append([scale*jax.random.normal(weight_key, shape = (out_width, in_width)),\n",
        "                       scale*jax.random.normal(bias_key, shape = (out_width,))\n",
        "                       ])\n",
        "    return params\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Pfln91dvWwp0"
      },
      "outputs": [],
      "source": [
        "@jit\n",
        "def MLP_predict(params, x):\n",
        "\n",
        "    hidden_layers = params[:-1]\n",
        "    activation = x\n",
        "\n",
        "    for w, b in hidden_layers:\n",
        "        activation = jax.nn.relu(jnp.dot(w, activation) + b)\n",
        "\n",
        "    w_last, b_last = params[-1]\n",
        "    logits = jnp.dot(w_last, activation) + b_last\n",
        "\n",
        "    return logits - logsumexp(logits)\n",
        "\n",
        "jit_MLP_predict = jit(MLP_predict)\n",
        "\n",
        "@jit\n",
        "def batched_MLP_predict(params,x):\n",
        "  return vmap(jit_MLP_predict, (None, 0))(params,x)\n",
        "  \n",
        "jit_batched_MLP_predict = jit(batched_MLP_predict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "hiCXqjmUTRUE"
      },
      "outputs": [],
      "source": [
        "Convu1_in = 32\n",
        "Convu2_in = 64\n",
        "kernelsize_ = (3,3)\n",
        "\n",
        "conv_init, conv_apply = stax.serial(\n",
        "    stax.Conv(Convu1_in,kernelsize_, padding = \"SAME\"),\n",
        "    stax.BatchNorm(),\n",
        "    stax.Relu,\n",
        "    stax.MaxPool((2,2)),\n",
        "    stax.Conv(Convu2_in, kernelsize_, padding = \"SAME\"),\n",
        "    stax.BatchNorm(),\n",
        "    stax.Relu,\n",
        "    stax.MaxPool((2,2)),\n",
        "    stax.Flatten,\n",
        "    stax.Dense(50),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "i4ULHJ0gctdT"
      },
      "outputs": [],
      "source": [
        "def random_cat_list(categories, number_different_catruns, number_training_categories):\n",
        "    combs = np.array(list(itertools.combinations(categories, number_training_categories)))\n",
        "    sample = np.random.randint(len(combs), size = number_different_catruns)\n",
        "    affe = np.array(combs[sample])\n",
        "    return affe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "q8Gf5Iy2w9Hv"
      },
      "outputs": [],
      "source": [
        "def load_best_weight_queue(pickle_path):\n",
        "  count = 0\n",
        "  best_weight_queue = []\n",
        "  base_bath = pickle_path.split(\"best_weight_\")[0]\n",
        "  for idx,i in enumerate(os.listdir(base_bath)[::-1]):\n",
        "    if \"best_weight\" in i and count < len_best_weight_queue:\n",
        "      \n",
        "      with open(base_bath+i, \"rb\") as input_file:\n",
        "        best_weight_queue.append(cPickle.load(input_file))\n",
        "      count = count+1\n",
        "  print(f\"Top {count} weights imported from Drive\")\n",
        "  return best_weight_queue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "YQEYcSNzVeim"
      },
      "outputs": [],
      "source": [
        "@jit\n",
        "def loss_fn(params, imgs, gt_lbls):\n",
        "  \n",
        "    predictions = jit_batched_MLP_predict(params, imgs)\n",
        "    #print(\"predictions\",predictions.shape)\n",
        "    return -jnp.mean(predictions * gt_lbls)\n",
        "    \n",
        "jit_loss_fn = jit(loss_fn)\n",
        "\n",
        "\n",
        "@jit\n",
        "def update(step, params, imgs, gt_lbls, opt_state,lr = 0.01):\n",
        "    loss, grads = value_and_grad(loss_fn)(params, imgs, gt_lbls)\n",
        "    opt_state = opt_update(step, grads, opt_state)\n",
        "\n",
        "    return loss, jax.tree_map(lambda p, g: p - lr*g, params, grads), opt_state\n",
        "\n",
        "'''@jit\n",
        "def update(params, imgs, gt_lbls,lr = 0.01):\n",
        "    loss, grads = value_and_grad(loss_fn)(params, imgs, gt_lbls)\n",
        "\n",
        "    return loss, jax.tree_map(lambda p, g: p - lr*g, params, grads)'''\n",
        "\n",
        "jit_update = jit(update)\n",
        "\n",
        "@jit\n",
        "def accuracy(conv_weights,MLP_params, dataset_imgs, dataset_lbls):\n",
        "\n",
        "    imgs = conv_apply(conv_weights, dataset_imgs.reshape(-1,data_image_size,data_image_size,data_image_depth))\n",
        "    pred_classes = jnp.argmax(jit_batched_MLP_predict(MLP_params, imgs.reshape(-1,NNin1)), axis = 1)\n",
        "\n",
        "    return jnp.mean(dataset_lbls ==   pred_classes+jnp.min(dataset_lbls))\n",
        "    \n",
        "jit_accuracy = jit(accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "NwD7MTmKD232"
      },
      "outputs": [],
      "source": [
        "'''For loop is neccesary to do batch training. Every update iteration needs to run with updated MPL params'''\n",
        "@jit\n",
        "def train(conv_weights, imgs, lbls,MLP_params,opt_state ):\n",
        "\n",
        "\n",
        "\n",
        "  for n in range(n_training_epochs):  \n",
        "    for i in range(jnp.shape(imgs)[0]):\n",
        "\n",
        "      gt_labels = jax.nn.one_hot(lbls[i], number_training_categories)\n",
        "      img_conv = conv_apply(conv_weights, imgs[i].reshape(-1,data_image_size,data_image_size,data_image_depth))\n",
        "      #loss, MLP_params = jit_update(MLP_params, img_conv.reshape(-1,NNin1), gt_labels)\n",
        "      step = (n+i)\n",
        "      loss, MLP_params,opt_state = jit_update(step, MLP_params, img_conv.reshape(-1,NNin1), gt_labels,opt_state)\n",
        "  return MLP_params\n",
        "  \n",
        "jit_train = jit(train)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "X3XQXryLAKXj"
      },
      "outputs": [],
      "source": [
        "@jit\n",
        "def bootstrapp_offspring_MLP(params,opt_state,conv_weights, batch_affe, labelaffe,test_images,test_lbls):\n",
        "  \n",
        "\n",
        "  MLP_params_trained = jit_train(conv_weights, batch_affe, labelaffe,\n",
        "                               #MLP_params,\n",
        "                               params,opt_state)\n",
        " \n",
        "  \n",
        "  result = jit_accuracy(conv_weights,MLP_params_trained,test_images,test_lbls)\n",
        "  return result,MLP_params_trained\n",
        "\n",
        "jit_bootstrapp_offspring_MLP = jit(bootstrapp_offspring_MLP)  \n",
        "\n",
        "@jit\n",
        "def vmap_bootstrapp_offspring_MLP(params,opt_state,conv_weights, batch_affe, labelaffe,test_images,test_lbls):\n",
        "  return vmap(jit_bootstrapp_offspring_MLP, ( None, None,None, 0,0,0,0))(params,opt_state,conv_weights, batch_affe, labelaffe,test_images,test_lbls)\n",
        "  \n",
        "jit_vmap_bootstrapp_offspring_MLP = jit(vmap_bootstrapp_offspring_MLP)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_varlist(results):\n",
        "  var_list = []\n",
        "  for i in results:\n",
        "    means_of_every_offspring = np.mean(i,axis = 2)\n",
        "    '''average standard deviation of the mean of every offspring, over all oepochs. How accurate is the fitness evaluation of one offspring?'''\n",
        "    var = np.var(means_of_every_offspring, axis = 0)\n",
        "    var_list.append(var)\n",
        "  return var_list"
      ],
      "metadata": {
        "id": "_Lu_TwDMuwLQ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gen_mean_std (var_list):\n",
        "  offspringmeans_overallruns = []\n",
        "  offspringsts_overallruns = []\n",
        "  for offspring_number in range(np.shape(var_list)[1]):\n",
        "    singleoff_varlist = []\n",
        "    for run_number in range(np.shape(var_list)[0]):\n",
        "      singleoff_varlist.append(var_list[run_number][offspring_number])\n",
        "    \n",
        "    mean, std = get_mean_and_std(singleoff_varlist)\n",
        "    offspringmeans_overallruns.append(mean)\n",
        "    offspringsts_overallruns.append(std)\n",
        "\n",
        "  total_generation_mean = np.mean(offspringmeans_overallruns)\n",
        "  total_generation_average_std = np.mean(np.array(offspringsts_overallruns)**2)**0.5\n",
        "  return total_generation_mean,total_generation_average_std"
      ],
      "metadata": {
        "id": "NyTY3kNPuSxZ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mean_and_std(fsingleoff_varlist):\n",
        "  fmean_std_off = (np.mean(fsingleoff_varlist))**0.5\n",
        "  count = 0\n",
        "  for i in fsingleoff_varlist:\n",
        "    count += (fmean_std_off-i**0.5)**2\n",
        "  fstd_meanstd_off = (count/len(fsingleoff_varlist))**0.5\n",
        "  return fmean_std_off, fstd_meanstd_off"
      ],
      "metadata": {
        "id": "_umcnTsluSxb"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logg_combi(test_combi):\n",
        "    file1 = open(combi_log_txt,\"a\")\n",
        "    file1.write(str(test_combi))\n",
        "    file1.write(\" - \")\n",
        "    file1.write(time.strftime(\"%d.%m.%Y_%H.%M\"))\n",
        "    file1.write(\"\\n\")\n",
        "    file1.close()\n",
        "\n",
        "def analysis_off_mean(results_overall):\n",
        "  affe = np.mean(np.moveaxis(results_overall, 1,0),axis = 2)\n",
        "  np.mean(np.var(affe, axis = 1))**0.5"
      ],
      "metadata": {
        "id": "kZ2NaA9OicpA"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "GnbL7xjrvZky"
      },
      "outputs": [],
      "source": [
        "'''Initialize Variables'''\n",
        "\n",
        "number_training_categories = 5\n",
        "NNin1 = 50\n",
        "NNout1 = number_training_categories\n",
        "\n",
        "googledrive_path = \"/content/drive/MyDrive/Colab Notebooks/\"\n",
        "file_name = \"Jax_Quickdraw_Experiment_3\"\n",
        "\n",
        "n_training_epochs = 10 #gradient descent iterations with same data for training the second network\n",
        "batch_size = 5\n",
        "n_offsp_epoch = 5 #repeat experiment with same offspring on different category sets\n",
        "\n",
        "\n",
        "'''keys'''\n",
        "starting_key = 260 #define starting point\n",
        "MLP_start_key = 3610 #seed \n",
        "numpy_seed = 8450 #in create offsprings\n",
        "\n",
        "use_pickle = False #load weights\n",
        "pickle_path = \"./pretrained_params/best_weight_0.8463.pkl\"\n",
        "use_best_weights = True"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main code"
      ],
      "metadata": {
        "id": "VcMeqQW6exiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(var_list):\n",
        "\n",
        "    std_modifier, n_runs,n_offsp_epoch, n_samples, n_test,number_different_catruns, comment = var_list\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "    '''Initialize variables'''\n",
        "    results_for_every_offspring = []\n",
        "    father_key = jax.random.PRNGKey(starting_key)\n",
        "    common_start_acc = 0\n",
        "    results_array = np.array([])\n",
        "    collect_stds = []\n",
        "    collect_means = []\n",
        "    collect_results = []\n",
        "    rng_MLP = jax.random.PRNGKey(MLP_start_key)\n",
        "    MLP_params = init_MLP([NNin1, NNout1], rng_MLP)\n",
        "    use_pickle = False\n",
        "    n_offsprings = 1\n",
        "    temp = 0.05\n",
        "    result_list = []\n",
        "    result_list_over_all_runs = []\n",
        "    avg_std_list = []\n",
        "\n",
        "    # Defining an optimizer in Jax\n",
        "    step_size = 1e-3\n",
        "    opt_init, opt_update, get_params = optimizers.adam(step_size)\n",
        "\n",
        "    '''Start Logging'''\n",
        "    save_path =  pathandstuff()\n",
        "    logg_script(file_name, save_path)\n",
        "    log_variables()\n",
        "    logg_GPU()\n",
        "    logg(comment)\n",
        "\n",
        "    start_meta = time.time()\n",
        "    start_overhead = time.time()\n",
        "\n",
        "    if use_pickle:\n",
        "      with open(pickle_path, \"rb\") as input_file:\n",
        "        father_weights = cPickle.load(input_file)\n",
        "        print(\"pickle weights imported\")\n",
        "      offspring_list,randomizer_list = jax_create_offsprings((numpy_seed),n_offsprings, father_weights,std_modifier)\n",
        "\n",
        "    else:\n",
        "      father_weights = conv_init(father_key, (batch_size,data_image_size,data_image_size,data_image_depth))\n",
        "      father_weights = father_weights[1] ## Weights are actually stored in second element of two value tuple\n",
        "      offspring_list,randomizer_list = jax_create_offsprings((numpy_seed),n_offsprings, father_weights,std_modifier)\n",
        "\n",
        "    results_list = []\n",
        "    '''Run multiple runs with the same offspring but different seeds and data'''\n",
        "    for t in tqdm(range(n_runs)):\n",
        "      results_overall = []\n",
        "\n",
        "      for oepoch in range(n_offsp_epoch):\n",
        "        random_state = (starting_key+oepoch)\n",
        "        result_list_metaepoch = []\n",
        "\n",
        "        random_training_categories = random_cat_list(known_list, number_different_catruns, number_training_categories)\n",
        "        x_train, _, y_train, _ = cat_dataloader(x_train_known_dataset,y_train_known_dataset,random_training_categories, random_state, n_samples, n_test, number_different_catruns)\n",
        "        _, x_test, _, y_test = cat_dataloader(x_test_known_dataset,y_test_known_dataset,random_training_categories, random_state, n_samples, n_test, number_different_catruns)\n",
        "\n",
        "        '''Cast to Jax Array'''\n",
        "        x_train = jnp.array(x_train,dtype = \"float32\").reshape(number_different_catruns,int((n_samples/batch_size)),batch_size,data_image_size,data_image_size)\n",
        "        x_test = jnp.array(x_test,dtype = \"float32\").reshape(number_different_catruns,n_test,data_image_size,data_image_size)\n",
        "        y_train = jnp.array(y_train).reshape(number_different_catruns,int((n_samples/batch_size)),batch_size)\n",
        "        y_test = jnp.array(y_test).reshape(number_different_catruns,n_test)\n",
        "\n",
        "        #print(\"Shape x_train\",np.shape(x_train))\n",
        "        #print(\"Shape y_train\",np.shape(y_train))\n",
        "        #print(\"Shape x_test\",np.shape(x_test))\n",
        "        #print(\"Shape y_test\",np.shape(y_test))\n",
        "\n",
        "        rng_MLP = jax.random.PRNGKey(random_state)\n",
        "        MLP_params = init_MLP([NNin1, NNout1], rng_MLP)\n",
        "\n",
        "        opt_state = opt_init(MLP_params)\n",
        "        params_with_adam = get_params(opt_state)\n",
        "\n",
        "        for i in range(len(offspring_list)):\n",
        "          conv_weights = offspring_list[i]\n",
        "          result_off,params_trained = jit_vmap_bootstrapp_offspring_MLP(params_with_adam,opt_state,conv_weights,x_train,y_train,x_test,y_test)\n",
        "          result_list_metaepoch.append(result_off)\n",
        "        results_overall.append(result_list_metaepoch)\n",
        "\n",
        "      \n",
        "      '''Assess Metaepoch'''\n",
        "      results_list.append(results_overall)\n",
        "      '''means over all cat combis for every offspring and oepoch'''\n",
        "      means_of_every_offspring = np.mean(results_overall,axis = 2)\n",
        "      '''average standard deviation of the mean of every offspring, over all oepochs. How accurate is the fitness evaluation of one offspring?'''\n",
        "      avg_std = np.mean(np.var(means_of_every_offspring, axis = 0))**0.5\n",
        "      avg_std_list.append(avg_std)\n",
        "      #logg(f\"\\taverage standard deviation of the mean of every offspring: {avg_std:.4f}\", )   \n",
        "      results_for_every_offspring = np.mean(np.mean(results_overall, axis = 0),axis = 1) #stimmt, checked 29.08\n",
        "      #logg(\"\\tMetaepoch mean: {:.4f}, std: {:.4f}\".format(np.mean(results_overall),np.std(results_for_every_offspring))) #stimmt, checked 29.08\n",
        "      #logg(\"\\tMetaepoch max performer: {:.4f}, min performer: {:.4f}\".format(np.max(results_for_every_offspring),np.min(results_for_every_offspring)))\n",
        "      #logg(\"\\tTime per metaepoch:{:.1f}s\\n\".format(time.time() - start_meta))\n",
        "\n",
        "    print()\n",
        "    print(\"+++++++++++++++++++\")\n",
        "    print(\"Assess Run\")\n",
        "    print(\"+++++++++++++++++++\")\n",
        "    print()\n",
        "\n",
        "    '''get string list of cat labels'''\n",
        "    string = \"\"\n",
        "    for i in known_npy:\n",
        "      string+= i+\", \"\n",
        "    string\n",
        "  \n",
        "    timestamp = time.strftime(\"%d.%m.%Y_%H.%M\")\n",
        "    ind_string = f\"N{n_runs}_T-{n_test}_std{std_modifier}\"\n",
        "    save_path = googledrive_path+costum_name+\"/\"+timestamp+\"_\"+ind_string+\"/\"\n",
        "\n",
        "\n",
        "    '''Start Logging'''\n",
        "    pathandstuff(save_path)\n",
        "    '''Save results after all n_runs'''\n",
        "    with open(save_path+\"results_overall_allruns.pkl\", 'wb') as f:\n",
        "      pickle.dump(result_list_over_all_runs, f, pickle.HIGHEST_PROTOCOL)\n",
        "      f.close()\n",
        "\n",
        "    '''deviation of the mean'''\n",
        "    mean, std = get_gen_mean_std (get_varlist(results_list))\n",
        "\n",
        "    '''Log to txt'''\n",
        "    logg(f\"average over n_runs: {n_runs}\")\n",
        "    logg(f\"average stds of mean over n_runs: {n_runs} -  {mean}\")\n",
        "    logg(\"number_different_catruns:\",number_different_catruns)\n",
        "    logg(\"avg_std_list:\",str(avg_std_list))\n",
        "    logg(\"std_modifier:\",std_modifier)\n",
        "    logg(f\"n_samples: {n_samples}\")\n",
        "    logg(f\"n_test: {n_test}\")\n",
        "    logg(f\"n_oepoch: {n_offsp_epoch}\")\n",
        "    logg(f\"average mean per metaepoch: {np.mean(np.mean(result_list_over_all_runs,dtype = object),dtype = object)}\")\n",
        "    logg(f\"category labels: {string}\")\n",
        "    logg(f\"n_offsprings: {n_offsprings}\")\n",
        "    logg(f\"number_different_catruns: {number_different_catruns}\")\n",
        "    logg(f\"temp: {temp}\")\n",
        "\n",
        "\n",
        "    '''load df'''\n",
        "    df_pickle_path = \"/content/drive/MyDrive/Colab Notebooks/\" + costum_name+ \"/df_stability_results_v2.pickle\"\n",
        "    with open(df_pickle_path, \"rb\") as input_file:\n",
        "          df = cPickle.load(input_file)\n",
        "\n",
        "\n",
        "    row_location = len(df.index)\n",
        "    df.loc[row_location,'n_runs'] = n_runs\n",
        "    df.loc[row_location,'std_modifier'] = std_modifier\n",
        "    df.loc[row_location,'average stds of mean '] = mean\n",
        "    df.loc[row_location,'average std of average stds of mean'] = std\n",
        "    df.loc[row_location,'time stamp'] = timestamp\n",
        "    df.loc[row_location,'n_samples'] = n_samples\n",
        "    df.loc[row_location,'n_test'] = n_test\n",
        "    df.loc[row_location,'average mean per metaepoch'] = np.mean(np.mean(result_list_over_all_runs,dtype = object),dtype = object)\n",
        "    df.loc[row_location,'category labels'] = string\n",
        "    df.loc[row_location,'n_offsprings'] = n_offsprings\n",
        "    df.loc[row_location,'n_offsp_epoch'] = n_offsp_epoch\n",
        "    df.loc[row_location,'Temperature'] = temp\n",
        "    df.loc[row_location,'number_different_catruns'] = number_different_catruns\n",
        "    df.loc[row_location,'time per run'] = (time.time()-start_n_run)/n_runs\n",
        "    df.loc[row_location,'comment'] = comment\n",
        "    print(df)\n",
        "    with open(df_pickle_path, 'wb') as f:\n",
        "        pickle.dump(df, f, pickle.HIGHEST_PROTOCOL)\n",
        "        f.close()\n",
        "\n",
        "    df.to_excel(r\"/content/drive/MyDrive/Colab Notebooks/\" + costum_name+ \"/stability_results_v2.xlsx\", index = False, encoding = \"UTF-8\")\n",
        "\n",
        "\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "PPjjggV6PxTi"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Defining an optimizer in Jax\n",
        "step_size = 1e-3\n",
        "opt_init, opt_update, get_params = optimizers.adam(step_size)"
      ],
      "metadata": {
        "id": "jHXBuDa1qAjR"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "costum_name = \"df_direc\"\n",
        "\n",
        "#Initialize pickle dataframe\n",
        "try:\n",
        "    os.mkdir(\"/content/drive/MyDrive/Colab Notebooks/\" + costum_name)\n",
        "except:\n",
        "  pass\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame()\n",
        "df_pickle_path = \"/content/drive/MyDrive/Colab Notebooks/\" + costum_name+ \"/df_stability_results_v2.pickle\"\n",
        "\n",
        "with open(df_pickle_path, 'wb') as f:\n",
        "    pickle.dump(df, f, pickle.HIGHEST_PROTOCOL)\n",
        "    f.close()\n",
        "\n",
        "df.to_excel(r\"/content/drive/MyDrive/Colab Notebooks/\" + costum_name+ \"/stability_results_v2.xlsx\", index = False, encoding = \"UTF-8\")\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/\" + costum_name+ \"/tested_combinations_v2.txt\", 'w') as f:\n",
        "    f.write(\"\")\n",
        "    f.close()\n"
      ],
      "metadata": {
        "id": "RpQ76WMsGXke"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combi_log_txt = \"/content/drive/MyDrive/Colab Notebooks/\" + costum_name+ \"/tested_combinations_v2.txt\"\n",
        "test_combi_list = []\n",
        "\n",
        "'''Create all possible combinations of parameter list'''\n",
        "#n_runs,n_metaepochs,n_offsp_epoch, n_samples, n_test,number_different_catruns, comment = var_list\n",
        "for xs in itertools.product([0.001], #std_modifier\n",
        "                            [50],  #n_runs \n",
        "                            [10,25,50], #n_offsp_epoch\n",
        "                            [25,100], #n_samples\n",
        "                            [75,150,300], #n_test\n",
        "                            [50,100,200], #number_different_catruns\n",
        "                            [\"check different oepoch and cat for 1 Offspring\"]): # comment                     \n",
        "  test_combi_list.append(xs)\n",
        "\n",
        "'''test every parameter combination'''\n",
        "for test_combi in test_combi_list:\n",
        "  '''load already tested combinations'''\n",
        "  tested_combis = []\n",
        "  with open(combi_log_txt, 'r') as fileobj:\n",
        "    for row in fileobj:\n",
        "      tested_combis.append(row.replace('\\n', '').replace('\\t', ''))\n",
        "  fileobj.close()\n",
        "\n",
        "  '''check if combi already tested, otherwise run test'''\n",
        "  if str(test_combi) not in '\\t'.join(tested_combis):\n",
        "    print(test_combi)\n",
        "    main(test_combi)\n",
        "    logg_combi(test_combi)\n"
      ],
      "metadata": {
        "id": "G1gU6xufpIr-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526,
          "referenced_widgets": [
            "d6aaf0cfee1945348c1f62d411510a91",
            "5a225cab2eb34b0fab423481d729af03",
            "c885ae0813b64e32bc375c877c845819",
            "0ab9b9a2d5df4198a925725482fa7a70",
            "0734144f53da4abb9c92300f74d62699",
            "8504d52345f9480fa0419321ba7f4d19",
            "494f4ce884a14e4d9d55a42fe4ca6093",
            "1d716a4032d3422ea085f19cce9fa4e9",
            "b29be8a50e704c92bd4ba10a2b24de65",
            "6ef4c724ca2e47da822a2dd0329ccdda",
            "6831ad6e6c824be580fe04f115a46bfd"
          ]
        },
        "outputId": "bfbb89af-c32d-4491-f0ff-60bcc8846b62"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.001, 50, 10, 25, 75, 50, 'check differen oepoch and cat for 1 offspring')\n",
            "on google\n",
            "Log path: /content/drive/MyDrive/Colab Notebooks/18.10.2022_19.57/\n",
            "n_training_epochs = 10\n",
            "n_offsp_epoch = 5\n",
            "GPU: Tesla T4\n",
            "check differen oepoch and cat for 1 offspring\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d6aaf0cfee1945348c1f62d411510a91"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-3e6e7e53199f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_combi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m'\\t'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtested_combis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_combi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_combi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mlogg_combi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_combi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-c76798e95f4d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(var_list)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mrandom_training_categories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_cat_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mknown_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_different_catruns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_training_categories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_known_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_known_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_training_categories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_different_catruns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_known_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test_known_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_training_categories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_different_catruns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;34m'''Cast to Jax Array'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-c2e3723874ef>\u001b[0m in \u001b[0;36mcat_dataloader\u001b[0;34m(x, y, random_categories, random_state, n_samples, n_test, number_different_catruns)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;34m'''Choose 5 random categories from data'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mx_data0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_categories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0my_data0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_categories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7wx6rvG2jwy1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}