{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-SzJ0NTKRP1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "from jax.scipy.special import logsumexp\n",
        "from numpy.random.mtrand import noncentral_chisquare\n",
        "from torch._C import NoneType\n",
        "import jax\n",
        "import _pickle as cPickle\n",
        "import pickle\n",
        "import copy\n",
        "from jax import jit, vmap, pmap, grad, value_and_grad\n",
        "import random\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.datasets import KMNIST\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torchvision.datasets import EMNIST\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from torch.utils.data import DataLoader\n",
        "from jax.example_libraries import stax, optimizers\n",
        "import torchvision\n",
        "import torch\n",
        "from numpy.random import default_rng\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import torch.utils.data as data_utils\n",
        "from jax.flatten_util import ravel_pytree\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import _pickle as cPickle\n",
        "from math import e\n",
        "import time\n",
        "from jax.example_libraries import stax\n",
        "from jax.example_libraries.stax import Dense, Relu, LogSoftmax\n",
        "from sklearn.model_selection import train_test_split\n",
        "from jax import random \n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "from logging import Logger\n",
        "import time\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zme13D7hbQxF",
        "outputId": "a4f1b314-3575-492d-ae3c-ec10aa548100"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FP3lGeSxvsO5"
      },
      "outputs": [],
      "source": [
        "known_npy = ['anvil',\n",
        " 'bathtub',\n",
        " 'bicycle',\n",
        " 'baseball bat',\n",
        " 'book',\n",
        " 'arm',\n",
        " 'asparagus',\n",
        " 'airplane',\n",
        " 'blackberry',\n",
        " 'bee',\n",
        " 'blueberry',\n",
        " 'barn',\n",
        " 'apple',\n",
        " 'banana',\n",
        " 'bench',\n",
        " 'bear',\n",
        " 'bandage',\n",
        " 'The Great Wall of China',\n",
        " 'basketball',\n",
        " 'basket']\n",
        "\n",
        "unknown_npy = ['beach',\n",
        " 'aircraft carrier',\n",
        " 'boomerang',\n",
        " 'axe',\n",
        " 'beard',\n",
        " 'bat',\n",
        " 'bird',\n",
        " 'ambulance',\n",
        " 'animal migration',\n",
        " 'The Mona Lisa',\n",
        " 'binoculars',\n",
        " 'bed',\n",
        " 'birthday cake',\n",
        " 'angel',\n",
        " 'alarm clock',\n",
        " 'belt',\n",
        " 'baseball',\n",
        " 'The Eiffel Tower',\n",
        " 'ant',\n",
        " 'backpack']\n",
        "\n",
        "\n",
        "if not any(\".npy\" in item for item in known_npy):\n",
        "  known_npy = [x+\".npy\" for x in known_npy]\n",
        "\n",
        "if not any(\".npy\" in item for item in unknown_npy):\n",
        "  unknown_npy = [x+\".npy\" for x in unknown_npy]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjKZxBoYvTQK"
      },
      "outputs": [],
      "source": [
        "'''Load data from npy files and normalize and standardize\n",
        "train_npy and test_npy set manually'''\n",
        "\n",
        "\n",
        "\n",
        "data_image_size = 28\n",
        "\n",
        "def normalize_standardize(arr):\n",
        "  arr = np.array(arr)/255\n",
        "  arr = (arr-arr.mean())/arr.std()\n",
        "  return arr\n",
        "\n",
        "\n",
        "x_train_known_dataset = []\n",
        "x_test_known_dataset = []\n",
        "y_train_known_dataset = []\n",
        "y_test_known_dataset = []\n",
        "\n",
        "x_train_unknown_dataset = []\n",
        "x_test_unknown_dataset = []\n",
        "y_train_unknown_dataset = []\n",
        "y_test_unknown_dataset = []\n",
        "\n",
        "known_list = []\n",
        "unknown_list = []\n",
        "\n",
        "data_image_size = 28\n",
        "data_image_depth = 1\n",
        "number_datapoints_npy = 5000\n",
        "data_filepath = \"./Quickdraw_data/\"\n",
        "#data_filepath = \"/content/drive/MyDrive/Colab Notebooks/Handdraw/data/\"\n",
        "\n",
        "\n",
        "\n",
        "if not any(\".npy\" in item for item in known_npy):\n",
        "  known_npy = [x+\".npy\" for x in known_npy]\n",
        "\n",
        "for idx, files in enumerate(os.listdir(data_filepath)):\n",
        "  if \"npy\" in files:\n",
        "    file_path = data_filepath+files\n",
        "    npy_load = np.load(file_path, encoding = 'latin1', allow_pickle = True)\n",
        "    rng = default_rng(idx)\n",
        "    idx_list = rng.choice(len(npy_load), size = number_datapoints_npy*2, replace = False)\n",
        "    idx_list_train = idx_list[number_datapoints_npy:]\n",
        "    idx_list_test = idx_list[:number_datapoints_npy]\n",
        "\n",
        "\n",
        "    if files in known_npy:\n",
        "\n",
        "      x_train_known_dataset.extend(npy_load[idx_list_train])\n",
        "      x_test_known_dataset.extend(npy_load[idx_list_test])\n",
        "      y_train_known_dataset.extend([idx]*number_datapoints_npy)\n",
        "      y_test_known_dataset.extend([idx]*number_datapoints_npy)\n",
        "      known_list.append(idx)\n",
        "\n",
        "    if files in unknown_npy:\n",
        "      x_train_unknown_dataset.extend(npy_load[idx_list_train])\n",
        "      x_test_unknown_dataset.extend(npy_load[idx_list_test])\n",
        "      y_train_unknown_dataset.extend([idx]*number_datapoints_npy)\n",
        "      y_test_unknown_dataset.extend([idx]*number_datapoints_npy)\n",
        "      unknown_list.append(idx)\n",
        "\n",
        "x_train_known_dataset = normalize_standardize(x_train_known_dataset).reshape(-1,data_image_size,data_image_size)\n",
        "y_train_known_dataset = np.array(y_train_known_dataset)\n",
        "x_test_known_dataset = normalize_standardize(x_test_known_dataset).reshape(-1,data_image_size,data_image_size)\n",
        "y_test_known_dataset = np.array(y_test_known_dataset)\n",
        "\n",
        "\n",
        "y_train_unknown_dataset = np.array(y_train_unknown_dataset)\n",
        "x_train_unknown_dataset = normalize_standardize(x_train_unknown_dataset).reshape(-1,data_image_size,data_image_size)\n",
        "y_test_unknown_dataset = np.array(y_test_unknown_dataset)\n",
        "x_test_unknown_dataset = normalize_standardize(x_test_unknown_dataset).reshape(-1,data_image_size,data_image_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwG6IZfAO9Y6"
      },
      "source": [
        "## **Funktions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXbzoV3qDgWh"
      },
      "source": [
        "Changes in Jax Jit function (sometimes?) need a cache restart for changes to be applied. Happened with def accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nXolMEPadfs"
      },
      "outputs": [],
      "source": [
        "def pathandstuff():\n",
        "\n",
        "    global save_txt\n",
        "    global base_path\n",
        "    global save_path\n",
        "\n",
        "    if os.path.exists(googledrive_path):\n",
        "        print(\"on google\")\n",
        "        base_path = googledrive_path\n",
        "    else:\n",
        "        raise ValueError('Please create folder for save path or connect to Google Drive')\n",
        "        \n",
        "    logs_path = base_path\n",
        "    '''Set logging and temp paths'''\n",
        "    timestamp = time.strftime(\"%d.%m.%Y_%H.%M\")\n",
        "    foldername = timestamp\n",
        "    save_path = os.path.join(logs_path,foldername,)\n",
        "    save_path = save_path+\"/\"\n",
        "    save_txt = os.path.join(save_path, 'Log_Jax_MNist_{}.txt'.format(foldername))\n",
        "      \n",
        "    if use_pickle:\n",
        "      save_path = pickle_path.split(\"best_weight_\")[0] \n",
        "      for i in os.listdir(save_path):\n",
        "        if \"Log\" in i:\n",
        "          save_txt = os.path.join(save_path, i)\n",
        "\n",
        "    print(\"Log path:\",save_path)\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "\n",
        "\n",
        "'''Save current notebook'''\n",
        "def logg_script(file_name, save_path):\n",
        "  source = f\"/content/drive/MyDrive/Colab Notebooks/{file_name}\"\n",
        "  destination = save_path+f\"{file_name}.ipynb\"\n",
        "  shutil.copy2(source, destination)\n",
        "\n",
        "'''logging to txt and print'''\n",
        "def logg(string_, array = None):\n",
        "  if array is None:\n",
        "\n",
        "    file1 = open(save_txt,\"a\")\n",
        "    file1.write(string_)\n",
        "    file1.write(\"\\n\")\n",
        "    file1.close()\n",
        "    print(string_)\n",
        "  if array is not None:\n",
        "\n",
        "    file1 = open(save_txt,\"a\")\n",
        "    file1.write(string_)\n",
        "    file1.write(str(array))\n",
        "    file1.write(\"\\n\")\n",
        "    file1.close()\n",
        "    print(string_, array)\n",
        "\n",
        "def log_variables():\n",
        "    try:\n",
        "    \n",
        "      logg((\"n_training_epochs = {}\".format(n_training_epochs)))\n",
        "      logg((\"n_offsp_epoch = {}\".format(n_offsp_epoch)))\n",
        "      logg((\"n_samples = {}\".format(n_samples)))\n",
        "      logg((\"n_test = {}\".format(n_test)))\n",
        "      logg((\"batch_size = {}\".format(batch_size)))\n",
        "      logg((\"std_modifier = {}\".format(std_modifier)))\n",
        "      logg((\"use_sigma_decay = {}\".format(use_sigma_decay)))\n",
        "      logg((\"sigma_start = {}\".format(sigma_start)))\n",
        "      logg((\"sigma_goal = {}\".format(sigma_goal)))\n",
        "      logg((\"n_decay_epochs = {}\".format(n_decay_epochs)))\n",
        "      logg((\"use_pickle = {}\".format(use_pickle)))\n",
        "      logg((\"pickle_path = {}\".format(pickle_path)))\n",
        "      logg((\"NNin1 = {}\".format(NNin1)))\n",
        "      logg((\"NNout1 = {}\".format(NNout1)))\n",
        "      logg((\"Convu_in1 = {}\".format(Convu1_in)))\n",
        "      logg((\"Convu2_in = {}\".format(Convu2_in)))\n",
        "      logg((\"Convu3_in = {}\".format(Convu3_in)))\n",
        "      logg((\"kernelsize_ = {}\".format(kernelsize_)))\n",
        "      logg((\"n_metaepochs = {}\".format(n_metaepochs)))   \n",
        "      logg((\"n_offsp_epoch = {}\".format(n_offsp_epoch)))\n",
        "      logg((\"n_offsprings = {}\".format(n_offsprings)))\n",
        "      logg((\"use_softmax = {}\".format(use_softmax)))\n",
        "      logg((\"temperature = {}\".format(temperature)))\n",
        "    except Exception:\n",
        "      pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RC9y3pZqqbwg"
      },
      "outputs": [],
      "source": [
        "def logg_GPU():\n",
        "  try:\n",
        "    affe=!nvidia-smi\n",
        "    string=\"\"\n",
        "    for i in affe:\n",
        "      string+=i\n",
        "    logg(\"GPU:\",string.split(\"=========||   0  \")[1].split(\"            Off  | 000\")[0])\n",
        "  except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Np2DOGve_e5E"
      },
      "outputs": [],
      "source": [
        "'''creating offsprings, filling treeleaf of 0 and 1 with gaussian noise, doesnt seem to be a problem, ex in offspring_list[0][5]'''\n",
        "\n",
        "\n",
        "def jax_create_offsprings(key,n_offspr,  fath_weights,std_var):\n",
        "  statedic_list = []\n",
        "  randomizer_list = []\n",
        "  for i in range(0,n_offspr):\n",
        "    std_modifier = std_var\n",
        "    rng = jax.random.PRNGKey(key+i)\n",
        "    if np.random.rand()<threshold_randomizer and use_std_randomizer:\n",
        "      std_modifier = std_var*factor_randomizer\n",
        "      randomizer_list.append(i)\n",
        "    random_value_tree = tree_random_normal_like(rng,fath_weights,std_modifier)\n",
        "    son = jax.tree_map(lambda x,y: x+y, fath_weights,random_value_tree)\n",
        "    statedic_list.append(son)\n",
        "  if use_std_randomizer:\n",
        "    return statedic_list, randomizer_list\n",
        "  else:\n",
        "    return statedic_list, []\n",
        "\n",
        "def random_split_like_tree(rng_key, target = None, treedef = None):\n",
        "    if treedef is None:\n",
        "        treedef = jax.tree_util.tree_structure(target)\n",
        "    keys = jax.random.split(rng_key, treedef.num_leaves)\n",
        "    return jax.tree_util.tree_unflatten(treedef, keys)\n",
        "\n",
        "\n",
        "def tree_random_normal_like(rng_key, target,std_modifier):\n",
        "    keys_tree = random_split_like_tree(rng_key, target)\n",
        "    return jax.tree_map(\n",
        "        lambda l, k: jax.random.normal(k, l.shape, l.dtype)*std_modifier,\n",
        "        target,\n",
        "        keys_tree,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Le4KpG9snz40"
      },
      "outputs": [],
      "source": [
        "'''softmax for offspring list\n",
        "    checked 11.04 working correctly'''\n",
        "def softmax_offlist(off_list,acc_list,temp):\n",
        "  softmax_list = softmax_result(acc_list,temp)\n",
        "  for i in range(len(off_list)):\n",
        "    if i == 0:\n",
        "      top_dog = jax.tree_map(lambda x: x*softmax_list[i], off_list[i])\n",
        "    else:\n",
        "      general_dog = jax.tree_map(lambda x: x*softmax_list[i], off_list[i])\n",
        "      top_dog = jax.tree_map(lambda x,y: x+y, top_dog,general_dog)\n",
        "  return top_dog\n",
        "\n",
        "\n",
        "'''Creates softmax/temp list out of accuracy list [0.2,0.3,....,0.8]'''\n",
        "def softmax_result(results,temp: float):\n",
        "    x = [z/temp for z in results]\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7PIGc8Y-zsc"
      },
      "outputs": [],
      "source": [
        "def sigma_decay(start, end, n_iter):\n",
        "  return(end/start)**(1/n_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgqKtdgZ-GZF"
      },
      "outputs": [],
      "source": [
        "def cat_dataloader(x,y,random_categories, random_state):\n",
        "  x_train = np.empty((n_samples*number_different_catruns,data_image_size,data_image_size))\n",
        "  x_test = np.empty((n_test*number_different_catruns,data_image_size,data_image_size))\n",
        "  y_train = np.empty((n_samples*number_different_catruns,))\n",
        "  y_test = np.empty((n_test*number_different_catruns,))\n",
        "\n",
        "  for i in range(number_different_catruns):\n",
        "\n",
        "    '''Choose 5 random categories from data'''\n",
        "    x_data0 = x[np.isin(y, random_categories[i]).flatten()]\n",
        "    y_data0 = y[np.isin(y, random_categories[i]).flatten()]\n",
        "\n",
        "    '''relabel categories to 0-5 to avoid accuracy bug in mlp'''\n",
        "    map_dic = dict(zip(random_categories[i], [0,1,2,3,4]))\n",
        "    y_data0 = np.vectorize(map_dic.get)(y_data0)\n",
        "\n",
        "    x_train_temp, x_test_temp, y_train_temp, y_test_temp = train_test_split(x_data0, y_data0, train_size = n_samples,\n",
        "                                                  test_size = n_test,stratify = y_data0,\n",
        "                                                  random_state = random_state)\n",
        "\n",
        "    x_train[i*n_samples:(i+1)*n_samples] = x_train_temp\n",
        "    x_test[i*n_test:(i+1)*n_test] = x_test_temp\n",
        "    y_train[i*n_samples:(i+1)*n_samples] = y_train_temp\n",
        "    y_test[i*n_test:(i+1)*n_test] = y_test_temp\n",
        "\n",
        "  '''Cast to Jax Array'''\n",
        "  x_train = jnp.array(x_train,dtype = \"float32\").reshape(len(x_train), -1)\n",
        "  x_test = jnp.array(x_test,dtype = \"float32\").reshape(len(x_test), -1)\n",
        "  y_train = jnp.array(y_train)\n",
        "  y_test = jnp.array(y_test)\n",
        "\n",
        "  '''Reshape for Jax Vectorization'''\n",
        "  x_train = x_train.reshape(-1,int((n_samples/batch_size)),batch_size,data_image_size,data_image_size)\n",
        "  y_train = y_train.reshape(-1,int((n_samples/batch_size)),batch_size)\n",
        "  x_test = x_test.reshape(-1,n_test,data_image_size,data_image_size)\n",
        "  y_test = y_test.reshape(-1,n_test)\n",
        "\n",
        "  return x_train, x_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4NrxSVjKt8f"
      },
      "outputs": [],
      "source": [
        "def init_MLP(layer_widths, parent_key, scale = 0.01):\n",
        "\n",
        "    params = []\n",
        "    keys = jax.random.split(parent_key, num = len(layer_widths)-1)\n",
        "\n",
        "    for in_width, out_width, key in zip(layer_widths[:-1], layer_widths[1:], keys):\n",
        "        weight_key, bias_key = jax.random.split(key)\n",
        "        params.append([scale*jax.random.normal(weight_key, shape = (out_width, in_width)),\n",
        "                       scale*jax.random.normal(bias_key, shape = (out_width,))\n",
        "                       ])\n",
        "    return params\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pfln91dvWwp0"
      },
      "outputs": [],
      "source": [
        "@jit\n",
        "def MLP_predict(params, x):\n",
        "\n",
        "    hidden_layers = params[:-1]\n",
        "    activation = x\n",
        "\n",
        "    for w, b in hidden_layers:\n",
        "        activation = jax.nn.relu(jnp.dot(w, activation) + b)\n",
        "\n",
        "    w_last, b_last = params[-1]\n",
        "    logits = jnp.dot(w_last, activation) + b_last\n",
        "\n",
        "    return logits - logsumexp(logits)\n",
        "\n",
        "jit_MLP_predict = jit(MLP_predict)\n",
        "\n",
        "@jit\n",
        "def batched_MLP_predict(params,x):\n",
        "  return vmap(jit_MLP_predict, (None, 0))(params,x)\n",
        "  \n",
        "jit_batched_MLP_predict = jit(batched_MLP_predict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiCXqjmUTRUE"
      },
      "outputs": [],
      "source": [
        "Convu1_in = 32\n",
        "Convu2_in = 64\n",
        "kernelsize_ = (3,3)\n",
        "\n",
        "conv_init, conv_apply = stax.serial(\n",
        "    stax.Conv(Convu1_in,kernelsize_, padding = \"SAME\"),\n",
        "    stax.BatchNorm(),\n",
        "    stax.Relu,\n",
        "    stax.MaxPool((2,2)),\n",
        "    stax.Conv(Convu2_in, kernelsize_, padding = \"SAME\"),\n",
        "    stax.BatchNorm(),\n",
        "    stax.Relu,\n",
        "    stax.MaxPool((2,2)),\n",
        "    stax.Flatten,\n",
        "    stax.Dense(50),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4ULHJ0gctdT"
      },
      "outputs": [],
      "source": [
        "def random_cat_list(categories, number_different_catruns, number_training_categories):\n",
        "    combs = np.array(list(itertools.combinations(categories, number_training_categories)))\n",
        "    sample = np.random.randint(len(combs), size = number_different_catruns)\n",
        "    affe = np.array(combs[sample])\n",
        "    return affe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8Gf5Iy2w9Hv"
      },
      "outputs": [],
      "source": [
        "def load_best_weight_queue(pickle_path):\n",
        "  count = 0\n",
        "  best_weight_queue = []\n",
        "  base_bath = pickle_path.split(\"best_weight_\")[0]\n",
        "  for idx,i in enumerate(os.listdir(base_bath)[::-1]):\n",
        "    if \"best_weight\" in i and count < len_best_weight_queue:\n",
        "      \n",
        "      with open(base_bath+i, \"rb\") as input_file:\n",
        "        best_weight_queue.append(cPickle.load(input_file))\n",
        "      count = count+1\n",
        "  print(f\"Top {count} weights imported from Drive\")\n",
        "  return best_weight_queue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQEYcSNzVeim"
      },
      "outputs": [],
      "source": [
        "@jit\n",
        "def loss_fn(params, imgs, gt_lbls):\n",
        "  \n",
        "    predictions = jit_batched_MLP_predict(params, imgs)\n",
        "    #print(\"predictions\",predictions.shape)\n",
        "    return -jnp.mean(predictions * gt_lbls)\n",
        "    \n",
        "jit_loss_fn = jit(loss_fn)\n",
        "\n",
        "\n",
        "@jit\n",
        "def update(step, params, imgs, gt_lbls, opt_state,lr = 0.01):\n",
        "    loss, grads = value_and_grad(loss_fn)(params, imgs, gt_lbls)\n",
        "    opt_state = opt_update(step, grads, opt_state)\n",
        "\n",
        "    return loss, jax.tree_map(lambda p, g: p - lr*g, params, grads), opt_state\n",
        "\n",
        "'''@jit\n",
        "def update(params, imgs, gt_lbls,lr = 0.01):\n",
        "    loss, grads = value_and_grad(loss_fn)(params, imgs, gt_lbls)\n",
        "\n",
        "    return loss, jax.tree_map(lambda p, g: p - lr*g, params, grads)'''\n",
        "\n",
        "jit_update = jit(update)\n",
        "\n",
        "@jit\n",
        "def accuracy(conv_weights,MLP_params, dataset_imgs, dataset_lbls):\n",
        "\n",
        "    imgs = conv_apply(conv_weights, dataset_imgs.reshape(-1,data_image_size,data_image_size,data_image_depth))\n",
        "    pred_classes = jnp.argmax(jit_batched_MLP_predict(MLP_params, imgs.reshape(-1,NNin1)), axis = 1)\n",
        "\n",
        "    return jnp.mean(dataset_lbls ==  pred_classes+jnp.min(dataset_lbls))\n",
        "    \n",
        "jit_accuracy = jit(accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwD7MTmKD232"
      },
      "outputs": [],
      "source": [
        "'''For loop is neccesary to do batch training. Every update iteration needs to run with updated MPL params'''\n",
        "@jit\n",
        "def train(conv_weights, imgs, lbls,MLP_params,opt_state ):\n",
        "\n",
        "\n",
        "\n",
        "  for n in range(n_training_epochs):  \n",
        "    for i in range(jnp.shape(imgs)[0]):\n",
        "\n",
        "      gt_labels = jax.nn.one_hot(lbls[i], number_training_categories)\n",
        "      img_conv = conv_apply(conv_weights, imgs[i].reshape(-1,data_image_size,data_image_size,data_image_depth))\n",
        "      #loss, MLP_params = jit_update(MLP_params, img_conv.reshape(-1,NNin1), gt_labels)\n",
        "      step = (n+i)\n",
        "      loss, MLP_params,opt_state = jit_update(step, MLP_params, img_conv.reshape(-1,NNin1), gt_labels,opt_state)\n",
        "  return MLP_params\n",
        "  \n",
        "jit_train = jit(train)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3XQXryLAKXj"
      },
      "outputs": [],
      "source": [
        "@jit\n",
        "def bootstrapp_offspring_MLP(params,opt_state,conv_weights, batch_affe, labelaffe,test_images,test_lbls):\n",
        "  \n",
        "\n",
        "  MLP_params_trained = jit_train(conv_weights, batch_affe, labelaffe,\n",
        "                               #MLP_params,\n",
        "                               params,opt_state)\n",
        " \n",
        "  \n",
        "  result = jit_accuracy(conv_weights,MLP_params_trained,test_images,test_lbls)\n",
        "  return result,MLP_params_trained\n",
        "\n",
        "jit_bootstrapp_offspring_MLP = jit(bootstrapp_offspring_MLP)  \n",
        "\n",
        "@jit\n",
        "def vmap_bootstrapp_offspring_MLP(params,opt_state,conv_weights, batch_affe, labelaffe,test_images,test_lbls):\n",
        "  return vmap(jit_bootstrapp_offspring_MLP, ( None, None,None, 0,0,0,0))(params,opt_state,conv_weights, batch_affe, labelaffe,test_images,test_lbls)\n",
        "  \n",
        "jit_vmap_bootstrapp_offspring_MLP = jit(vmap_bootstrapp_offspring_MLP)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnbL7xjrvZky"
      },
      "outputs": [],
      "source": [
        "'''Initialize Variables'''\n",
        "\n",
        "number_training_categories = 5\n",
        "NNin1 = 50\n",
        "NNout1 = number_training_categories\n",
        "\n",
        "\n",
        "googledrive_path = \"/content/drive/MyDrive/Colab Notebooks/\"\n",
        "file_name = \"Jax_Quickdraw_Experiment_2\"\n",
        "\n",
        "n_metaepochs = 5000 #Number of generations\n",
        "n_offsprings = 50\n",
        "n_training_epochs = 10 #gradient descent iterations with same data for training the second network\n",
        "n_samples = 25 #n of training independent training samples for 2nd network - MLP, samples are stratified\n",
        "batch_size = 5\n",
        "number_different_catruns = 50 #number of second networks with different categories to take average performance of. \n",
        "n_offsp_epoch = 5 #repeat experiment with same offspring on different category sets\n",
        "\n",
        "n_test = 75\n",
        "test_every_x_meta = 10 #how often test for generalization\n",
        "\n",
        "\n",
        "'''keys'''\n",
        "starting_key = 260 #define starting point\n",
        "MLP_start_key = 3610 #seed \n",
        "numpy_seed = 8450 #in create offsprings\n",
        "\n",
        "use_n_elitist = False #weight update method\n",
        "n_elitist = 10\n",
        "\n",
        "use_Softmax = True #weight update method\n",
        "use_father = True\n",
        "std_modifier = 0.01\n",
        "temp = 0.01\n",
        "\n",
        "use_elitist = False #weight update method\n",
        "\n",
        "use_sigma_decay = True #otherwise using constant sigma from config tab, decreasing sigma for random noise over time\n",
        "n_decay_epochs = 500   # over how many metaepochs sigma is decayed\n",
        "sigma_start = 0.05\n",
        "sigma_goal = 0.01 #sigma goal after n_metaepochs\n",
        "\n",
        "\n",
        "use_temp_decay = True\n",
        "n_decay_epochs_temp = 500\n",
        "temp_start = 0.05\n",
        "temp_goal = 0.01\n",
        "\n",
        "use_pickle = False #load weights\n",
        "pickle_path = \"./pretrained_params/best_weight_0.8463.pkl\"\n",
        "use_best_weights = True\n",
        "\n",
        "use_best_weight_queue = True #keep track of the n best weights and use in every metaepoch\n",
        "len_best_weight_queue = 10 \n",
        "\n",
        "\n",
        "use_best_father_queue = True\n",
        "len_best_father_queue = 10\n",
        "\n",
        "use_std_randomizer = True\n",
        "threshold_randomizer = 0.1\n",
        "factor_randomizer = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfwTthsBbg_1"
      },
      "source": [
        "\n",
        "# Main code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bvkbfqNBOmQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "outputId": "2eb69b56-abcc-4408-e31f-ea8770d1642e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "on google\n",
            "Log path: /content/drive/MyDrive/Colab Notebooks/18.10.2022_18.56/\n",
            "n_training_epochs = 10\n",
            "n_offsp_epoch = 5\n",
            "n_samples = 25\n",
            "n_test = 75\n",
            "batch_size = 5\n",
            "std_modifier = 0.01\n",
            "use_sigma_decay = True\n",
            "sigma_start = 0.05\n",
            "sigma_goal = 0.01\n",
            "n_decay_epochs = 500\n",
            "use_pickle = False\n",
            "pickle_path = ./pretrained_params/best_weight_0.8463.pkl\n",
            "NNin1 = 50\n",
            "NNout1 = 5\n",
            "Convu_in1 = 32\n",
            "Convu2_in = 64\n",
            "GPU: Tesla T4\n",
            "\tload known data for metaepoch:0\n",
            "\taverage standard deviation of the mean of every offspring: 0.0196\n",
            "\tMetaepoch mean: 0.4787, std: 0.0173\n",
            "\tMetaepoch max performer: 0.5073, min performer: 0.4028\n",
            "\tTime per metaepoch:76.0s\n",
            "\n",
            "New best performer mean: 0.5073\n",
            "\tload known data for metaepoch:1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-0f73cc5367b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstarting_key\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0moepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_known_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_known_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_training_categories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_known_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test_known_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_training_categories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-5dc1e40b48e9>\u001b[0m in \u001b[0;36mcat_dataloader\u001b[0;34m(x, y, random_categories, random_state)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;34m'''Choose 5 random categories from data'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mx_data0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_categories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0my_data0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_categories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "'''Start Logging'''\n",
        "pathandstuff()\n",
        "logg_script(file_name, save_path)\n",
        "log_variables()\n",
        "logg_GPU()\n",
        "\n",
        "\n",
        "'''Initialize variables'''\n",
        "rng_MLP = jax.random.PRNGKey(MLP_start_key)\n",
        "results_for_every_offspring = []\n",
        "best_performer = 0\n",
        "father_key = jax.random.PRNGKey(starting_key)\n",
        "best_weights = conv_init(father_key, (batch_size,data_image_size,data_image_size,data_image_depth))[1]\n",
        "common_start_acc = 0\n",
        "best_weight_queue = []\n",
        "best_father_queue = []\n",
        "best_father_mean = 0\n",
        "randomizer_list = []\n",
        "\n",
        "MLP_params = init_MLP([NNin1, NNout1], rng_MLP)\n",
        "\n",
        "# Defining an optimizer in Jax\n",
        "step_size = 1e-3\n",
        "opt_init, opt_update, get_params = optimizers.adam(step_size)\n",
        "\n",
        "for meta in range (n_metaepochs):\n",
        "    start_meta = time.time()\n",
        "\n",
        "    '''Create category pairs'''\n",
        "    random_training_categories = random_cat_list(known_list, number_different_catruns, number_training_categories)\n",
        "    random_testing_categories = random_cat_list(unknown_list, number_different_catruns, number_training_categories)\n",
        "\n",
        "    \n",
        "    if use_sigma_decay:\n",
        "      sigma_base = sigma_decay(sigma_start, sigma_goal, n_decay_epochs)\n",
        "      if meta < n_decay_epochs:\n",
        "        std_modifier = sigma_start*sigma_base**meta\n",
        "      else:\n",
        "        std_modifier = sigma_start*sigma_base**n_decay_epochs\n",
        "\n",
        "\n",
        "    if use_temp_decay:\n",
        "      temp_base = sigma_decay(temp_start, temp_goal, n_decay_epochs_temp)\n",
        "      if meta < n_decay_epochs_temp:\n",
        "        temp = temp_start*temp_base**meta\n",
        "      else:\n",
        "        temp = temp_start*temp_base**n_decay_epochs_temp\n",
        "\n",
        "    '''Starting point'''\n",
        "    if meta == 0:\n",
        "\n",
        "      if use_pickle:\n",
        "        with open(pickle_path, \"rb\") as input_file:\n",
        "          father_weights = cPickle.load(input_file)\n",
        "          print(\"pickle weights imported\") \n",
        "        offspring_list,randomizer_list = jax_create_offsprings((meta+numpy_seed),n_offsprings, father_weights,std_modifier)\n",
        "        if use_father:\n",
        "          offspring_list[0] = father_weights\n",
        "\n",
        "        if use_best_weight_queue:\n",
        "          best_weight_queue = load_best_weight_queue(pickle_path)\n",
        "          offspring_list.extend(best_weight_queue)\n",
        "        \n",
        "        save_path = pickle_path.split(\"best_weight_\")[0]\n",
        "        \n",
        "\n",
        "      else:\n",
        "\n",
        "        father_weights = conv_init(father_key, (batch_size,data_image_size,data_image_size,data_image_depth))\n",
        "        father_weights = father_weights[1] ## Weights are actually stored in second element of two value tuple\n",
        "        offspring_list,randomizer_list = jax_create_offsprings((meta+numpy_seed),n_offsprings, father_weights,std_modifier)\n",
        "        if use_father:\n",
        "          offspring_list[0] = father_weights\n",
        "\n",
        "\n",
        "    '''Weight updates'''      \n",
        "    if meta >= 1:\n",
        "\n",
        "        '''Softmax Update'''\n",
        "        \n",
        "        if use_Softmax and not ((meta-1) % (test_every_x_meta) ==  0): #Disable weight update for testing epoch\n",
        "          #print(\"using softmax\")\n",
        "          \n",
        "          father_weights = softmax_offlist(offspring_list,results_for_every_offspring,temp)\n",
        "          offspring_list,randomizer_list = jax_create_offsprings((meta+numpy_seed),n_offsprings, father_weights,std_modifier)\n",
        "          if use_father:\n",
        "            offspring_list[0] = father_weights\n",
        "            offspring_list[1] = best_weights\n",
        "            \n",
        "        if use_elitist and not ((meta-1) % (test_every_x_meta) ==  0):\n",
        "          \n",
        "          best_offspring = offspring_list[(np.argmax(results_for_every_offspring))]\n",
        "          offspring_list, randomizer_list = jax_create_offsprings((meta+numpy_seed),n_offsprings, best_offspring,std_modifier)\n",
        "          if use_father:\n",
        "              offspring_list[0] = father_weights\n",
        "              offspring_list[1] = best_weights\n",
        "              \n",
        "        if use_n_elitist and not ((meta-1) % (test_every_x_meta) ==  0):\n",
        "                      \n",
        "          offspring_list_help = []\n",
        "\n",
        "          indices = (-results_for_every_offspring).argsort()[:n_elitist]\n",
        "          for idx in indices:\n",
        "            help,helper_randomizer_list = jax_create_offsprings((meta+numpy_seed),int(n_offsprings/n_elitist), offspring_list[idx],std_modifier)\n",
        "            randomizer_list.extend(helper_randomizer_list*n_elitist*idx)\n",
        "            offspring_list_help.extend(help)\n",
        "          if use_father:\n",
        "\n",
        "              offspring_list[1] = best_weights\n",
        "          offspring_list = offspring_list_help  \n",
        "          \n",
        "        if use_best_weight_queue:\n",
        "           offspring_list.extend(best_weight_queue)     \n",
        "\n",
        "        if use_best_father_queue:\n",
        "          offspring_list.extend(best_father_queue)  \n",
        "            \n",
        "    results_overall = []\n",
        "    #print(\"Overhead meta time:\", time.time() - start_overhead)\n",
        "    for oepoch in range(n_offsp_epoch):\n",
        "      rng_MLP = jax.random.PRNGKey(MLP_start_key+meta+oepoch)  \n",
        "      MLP_params = init_MLP([NNin1, NNout1], rng_MLP)\n",
        "      start_data_collection_time = time.time()\n",
        "      result_list_metaepoch = []\n",
        "      '''Load \"known data\" for test_every_x_meta episodes'''\n",
        "      if (meta % test_every_x_meta !=  0 or meta ==  0):\n",
        "        if oepoch ==  0:\n",
        "          logg(f\"\\tload known data for metaepoch:{meta}\")\n",
        "        random_state = (starting_key+meta+oepoch)\n",
        "\n",
        "        x_train, _, y_train, _ = cat_dataloader(x_train_known_dataset,y_train_known_dataset,random_training_categories, random_state)\n",
        "        _, x_test, _, y_test = cat_dataloader(x_test_known_dataset,y_test_known_dataset,random_training_categories, random_state)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        '''Load \"unknown data\" every test_every_x_meta run'''\n",
        "      else:\n",
        "        if oepoch ==  0:\n",
        "          logg(f\"\\tload unknown data for metaepoch:{meta}\")\n",
        "        random_state = (starting_key+meta+oepoch)\n",
        "        x_train, _, y_train, _ = cat_dataloader(x_train_unknown_dataset,y_train_unknown_dataset,random_testing_categories, random_state)\n",
        "        _, x_test, _, y_test = cat_dataloader(x_test_unknown_dataset,y_test_unknown_dataset,random_testing_categories, random_state)\n",
        "\n",
        "        '''Cast to Jax Array'''\n",
        "      x_train = jnp.array(x_train,dtype = \"float32\").reshape(number_different_catruns,int((n_samples/batch_size)),batch_size,data_image_size,data_image_size)\n",
        "      x_test = jnp.array(x_test,dtype = \"float32\").reshape(number_different_catruns,n_test,data_image_size,data_image_size)\n",
        "      y_train = jnp.array(y_train).reshape(number_different_catruns,int((n_samples/batch_size)),batch_size)\n",
        "      y_test = jnp.array(y_test).reshape(number_different_catruns,n_test)\n",
        "      \n",
        "      opt_state = opt_init(MLP_params)\n",
        "      params_with_adam = get_params(opt_state)\n",
        "\n",
        "\n",
        "      #print(\"Shape x_train\",np.shape(x_train))\n",
        "      #print(\"Shape y_train\",np.shape(y_train))\n",
        "      #print(\"Shape x_test\",np.shape(x_test))\n",
        "      #print(\"Shape y_test\",np.shape(y_test))\n",
        "      \n",
        "      #print(\"\\tLÃ¤nge Offspring List:\",len(offspring_list))\n",
        "      #print(\"Overhead data time:\", time.time() - start_data_collection_time)\n",
        "      for i in range(len(offspring_list)):\n",
        "        \n",
        "        conv_weights = offspring_list[i]\n",
        "        \n",
        "        result_off,params_trained = jit_vmap_bootstrapp_offspring_MLP(params_with_adam,opt_state,conv_weights,x_train,y_train,x_test,y_test)\n",
        "        result_list_metaepoch.append(result_off)\n",
        "      results_overall.append(result_list_metaepoch)\n",
        "\n",
        "\n",
        "    '''Assess Metaepoch'''\n",
        "    '''means over all cat combis for every offspring and oepoch'''\n",
        "    means_of_every_offspring = np.mean(results_overall,axis = 2)\n",
        "    '''average standard deviation of the mean of every offspring, over all oepochs. How accurate is the fitness evaluation of one offspring?'''\n",
        "    avg_std = np.mean(np.var(means_of_every_offspring, axis = 0))**0.5\n",
        "\n",
        "\n",
        "    logg(f\"\\taverage standard deviation of the mean of every offspring: {avg_std:.4f}\", ) \n",
        "    results_for_every_offspring = np.mean(np.mean(results_overall, axis = 0),axis = 1) #stimmt, checked 29.08\n",
        "    logg(\"\\tMetaepoch mean: {:.4f}, std: {:.4f}\".format(np.mean(results_overall),np.std(results_for_every_offspring))) #stimmt, checked 29.08\n",
        "    logg(\"\\tMetaepoch max performer: {:.4f}, min performer: {:.4f}\".format(np.max(results_for_every_offspring),np.min(results_for_every_offspring)))\n",
        "    logg(\"\\tTime per metaepoch:{:.1f}s\\n\".format(time.time() - start_meta))\n",
        "\n",
        "    if any(item in randomizer_list for item in np.argpartition(results_for_every_offspring, -5)[-5:]):\n",
        "      print(\"\\t+++Top 5 results include randomizer offspring+++\")\n",
        "\n",
        "    '''Check for best performer'''\n",
        "    metaepoch_superhero = None\n",
        "    for idx, acc_value in enumerate(results_for_every_offspring):\n",
        "        if acc_value>best_performer:\n",
        "          best_performer = acc_value\n",
        "          metaepoch_superhero = idx\n",
        "    if metaepoch_superhero:\n",
        "      if metaepoch_superhero in randomizer_list:\n",
        "        print(\"+++Best weight delivered by randomizer+++\")\n",
        "      best_performer = results_for_every_offspring[metaepoch_superhero]\n",
        "      best_weights = offspring_list[metaepoch_superhero]\n",
        "      with open(save_path+f\"best_weight_{best_performer:.4f}.pkl\", 'wb') as f:\n",
        "        pickle.dump(best_weights, f, pickle.HIGHEST_PROTOCOL)\n",
        "        f.close()\n",
        "      logg(f\"New best performer mean: {best_performer:.4f}\")\n",
        "\n",
        "      '''FIFO for best weight queue, only one item per metaepoch is added to increase diversity'''\n",
        "      if use_best_weight_queue:\n",
        "        best_weight_queue.append(offspring_list[metaepoch_superhero])\n",
        "        if len(best_weight_queue)> len_best_weight_queue:\n",
        "          best_weight_queue = best_weight_queue[-len_best_weight_queue:]\n",
        "\n",
        "\n",
        "    if np.mean(results_overall) > best_father_mean:\n",
        "      best_father_mean = np.mean(results_overall)\n",
        "      best_father_queue.append(father_weights)\n",
        "      if len(best_father_queue)> len_best_father_queue:\n",
        "        best_father_queue = best_father_queue[-len_best_father_queue:]\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
